---
title: "サーバーレスGPU環境のModalを試す"
summary: "サーバーレスGPU環境のModalを試してみました。"
date: 2025-05-02
lastmod: 2025-05-02
draft: false
authors: ["caru"]
tags: ["development", "modal", "gpu", "serverless"]
---

どうも、[Caru](https://runa.dev/caru)です。

GPUのレンタルって、環境構築の手間だったり、使わない時間もコストがかさんだり、結構大変ですよね。
最近は、Vast.ai ([https://vast.ai](https://vast.ai)) や RunPod ([https://runpod.io](https://runpod.io)) のような、もっと手軽にGPUパワーを使えるサーバーレスなプラットフォームが登場してきています。
今回はその中でも特に、開発体験が良さそうだと感じた「Modal」について、どんなサービスなのかご紹介したいと思います。

## Modalとは

Modal ([https://modal.com](https://modal.com)) は、一言でいうと「Pythonコードをデコレータ一つでクラウド実行できちゃう」サーバーレスプラットフォームです。
特にGPUが必要な機械学習の推論やバッチ処理なんかに強みを持っています。
インフラの面倒な設定はほとんど不要で、普段Pythonを書く延長線上でクラウドのリソース、特にGPUをサクッと使えるのが大きな特徴です。

## 簡単なコード例

じゃあ、実際にどんな感じでコードを書くのか、ModalのドキュメントにあるHello Worldの例を見てみましょう。
[出典: Modal Docs - Hello, world!](https://modal.com/docs/examples/hello_world)

```python
import sys
import modal

# アプリケーションを定義
app = modal.App("example-hello-world")

# Modalで実行する関数を定義
@app.function()
def f(i):
    if i % 2 == 0:
        print("hello", i)
    else:
        # 標準エラー出力はModalのUI上で警告として表示されます
        print("world", i, file=sys.stderr)
    return i * i

# ローカルから実行するためのエントリーポイントを定義
@app.local_entrypoint()
def main():
    # 関数をローカルで実行
    print("f.local(1000) =", f.local(1000))

    # 関数をModalのクラウド上でリモート実行
    print("f.remote(1000) =", f.remote(1000))

    # 複数の入力に対して関数を並列実行 (Map)
    total = 0
    # range(20) の各数値に対して f() を並列実行し、結果をイテレート
    for ret in f.map(range(20)):
        total += ret
    print("f.map(range(20)) total =", total)

```

`app = modal.App()` でアプリケーションを定義します。
`@app.function()` デコレータを付けることで、関数 `f` がModalのクラウド上で実行可能になります。
ローカルで試す場合は `f.local()`、Modal上で実行する場合は `f.remote()` を使います。
さらに `.map()` を使うことで、複数の入力に対して関数を簡単に並列実行できます。

`@app.local_entrypoint()` デコレータは、`modal run` コマンドでこのスクリプトを実行したときに `main()` 関数が呼び出されるように指定します。

このように、普段Pythonを書くのとほとんど変わらない記述で、簡単にクラウドのリソースを活用できるのがModalの魅力です。

## 何がうれしいか

まず、起動がめちゃくちゃ速いです。
Modalが独自に開発したコンテナ技術のおかげで、ローカルで開発するのと同じような感覚でクラウド上のリソースを使えるとのこと。待ち時間が少ないのは正義！

設定ファイル地獄からも解放されます。ハードウェアの指定なんかもPythonコードのすぐ近くに書けるので、あちこち設定ファイルを見に行く必要がありません。

そして、スケーリングがすごい。必要になったら一瞬で数百のGPUまでスケールして、処理が終わればゼロになる。つまり、本当に使った分だけのコストで済むわけです。これはお財布に優しい！

あと、UIがめちゃくちゃいい。ここ最近使ったSaaSの中ではトップクラスで見やすいし、ドキュメントもかなり丁寧です。

## どんなことに使える？

Modalのサイトを見ると、やはり最新のAI関連、特に生成AIでの利用例が目立ちますね。
例えば、**大規模言語モデル (LLM) のAPIサーバー**を立てたり、**Stable Diffusion (SDXL) やFLUXといった画像生成モデル**、**音楽生成や動画生成モデル**を実行するための基盤としても活用されているようです。  

もちろん、**モデルのファインチューニング**や、大量のデータを処理する**バッチジョブ**といった、従来の機械学習ワークロードにも対応しています。
アイデア次第で色々なことに応用できそうですが、特にGPUパワーを必要とする最新のAIモデルを手軽に動かしたい場合に強力な選択肢となりそうです。

## 料金

料金は、CPU、メモリ、GPUを使った時間（秒単位！）に応じた従量課金制です。
ありがたいことに、毎月$30の無料枠が付いてくるので、個人で試したり、小規模なプロジェクトで使う分には十分かもしれません。太っ腹！

GPUの料金も、それほど高くないので、開発のしやすさなどを考えたら、かなりありなのではと思います。
(公式サイトの[Compute costs](https://modal.com/pricing#compute-costs)を参照)

| Compute costs | Per second | Per hour |
|---------------|------------|----------|
| **GPU Tasks** |            |          |
| Nvidia H100 | $0.001097 / sec | $3.95 |
| Nvidia A100, 80 GB | $0.000694 / sec | $2.50 |
| Nvidia A100, 40 GB | $0.000583 / sec | $2.10 |
| Nvidia L40S | $0.000542 / sec | $1.95 |
| Nvidia A10G | $0.000306 / sec | $1.10 |
| Nvidia L4 | $0.000222 / sec | $0.80 |
| Nvidia T4 | $0.000164 / sec | $0.59 |
| **CPU** |            |          |
| Physical core (2 vCPU equivalent)* | $0.0000131 / core / sec | $0.047 |
| **Memory** | $0.00000222 / GiB / sec | $0.008 |

*minimum of 0.125 cores per container

## まとめ

というわけで、今回はサーバーレスGPUサービスのModalを紹介しました。
GPU環境のセットアップやコスト管理に悩んでいた人にとっては、かなり魅力的な選択肢になるんじゃないでしょうか。
「この処理、ローカルじゃキツイな…」と思ったときに、デコレータを追加するだけでクラウドのパワーを借りられるのは体験として最高ですよね。

僕もこれから本格的に使ってみようと思っています。興味を持った方はぜひ公式サイトをチェックしてみてください！
この分野はこれからますます面白くなりそうですね。

- Modal公式サイト

https://modal.com

- Modalのドキュメント

https://modal.com/docs
